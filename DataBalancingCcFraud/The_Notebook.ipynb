{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrjgoos7ndHq"
   },
   "source": [
    "Importing the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWWAMaWylijD"
   },
   "source": [
    "# Welcome to the notebook ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FF-XdZOlrBz"
   },
   "source": [
    "### Task 1 - Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:35.389480Z",
     "start_time": "2024-08-06T16:07:31.971532Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rg59XDzGlmNx",
    "outputId": "298c1fa1-fdd4-4a0c-e3cf-fde4bc95ffc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.9)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.47.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 16:12:22.420684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules are imported!\n"
     ]
    }
   ],
   "source": [
    "# Installing TensorFlow\n",
    "!pip install tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing neural network modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# Importing some machine learning modules\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import data visualization modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "print(\"Modules are imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:35.571617Z",
     "start_time": "2024-08-06T16:07:35.391145Z"
    },
    "id": "QulLdZ53nqBF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "1     7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
      "2    10  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152 -1.423236   \n",
      "3    10  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027  0.470455   \n",
      "4    11  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544 -0.096717   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "1 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
      "2  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512  0.251367   \n",
      "3  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710 -0.767315   \n",
      "4  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744  0.548265   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.221929  0.062723  0.061458  123.50      0  \n",
      "1 -0.051634 -1.206921 -1.085339   40.80      0  \n",
      "2 -0.129478  0.042850  0.016253    7.80      0  \n",
      "3 -0.492208  0.042472 -0.054337    9.99      0  \n",
      "4  0.104094  0.021491  0.021293   27.50      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F61uPjLozPzn"
   },
   "source": [
    "Check the data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:35.579010Z",
     "start_time": "2024-08-06T16:07:35.573675Z"
    },
    "id": "ie9B694UlDok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "1     7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
      "2    10  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152 -1.423236   \n",
      "3    10  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027  0.470455   \n",
      "4    11  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544 -0.096717   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "1 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
      "2  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512  0.251367   \n",
      "3  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710 -0.767315   \n",
      "4  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744  0.548265   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.221929  0.062723  0.061458  123.50      0  \n",
      "1 -0.051634 -1.206921 -1.085339   40.80      0  \n",
      "2 -0.129478  0.042850  0.016253    7.80      0  \n",
      "3 -0.492208  0.042472 -0.054337    9.99      0  \n",
      "4  0.104094  0.021491  0.021293   27.50      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Dataset columns: Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "Dataset shape: (50492, 31)\n"
     ]
    }
   ],
   "source": [
    "# verify the data\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(data.head())\n",
    "\n",
    "# Display the columns of the data\n",
    "print(f\"Dataset columns: {data.columns}\")\n",
    "\n",
    "# Display the shape of the data\n",
    "print(f\"Dataset shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:35.586302Z",
     "start_time": "2024-08-06T16:07:35.580801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after cleaning: (50492, 31)\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "data = data.dropna()\n",
    "\n",
    "# Display the shape of the data\n",
    "print(f\"Dataset shape after cleaning: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:58.939077Z",
     "start_time": "2024-08-06T16:07:35.587106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "Column 'correct_numerical_column' does not exist in the DataFrame. Available columns are: Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Creditcard_dataset.csv')\n",
    "\n",
    "# Verify the column names in the DataFrame\n",
    "print(f\"Dataset columns: {data.columns}\")\n",
    "\n",
    "# Specify the correct column names\n",
    "categorical_column = 'correct_categorical_column'\n",
    "numerical_column = 'correct_numerical_column'\n",
    "\n",
    "# Check if the numerical column exists\n",
    "if numerical_column not in data.columns:\n",
    "    print(f\"Column '{numerical_column}' does not exist in the DataFrame. Available columns are: {data.columns}\")\n",
    "else:\n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    data[[numerical_column]] = scaler.fit_transform(data[[numerical_column]])\n",
    "\n",
    "# Check if the categorical column exists\n",
    "if categorical_column not in data.columns:\n",
    "    # If the column is missing, use a machine learning model to predict the missing values\n",
    "    # Assuming 'Class' is the target column and we have other features to predict the missing column\n",
    "    if 'Class' in data.columns:\n",
    "        # Split the data into features and target\n",
    "        X = data.drop(columns=['Class'])\n",
    "        y = data['Class']\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train a RandomForestClassifier to predict the missing column\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the missing column values\n",
    "        data[categorical_column] = clf.predict(data.drop(columns=['Class']))\n",
    "    else:\n",
    "        print(f\"Column '{categorical_column}' does not exist in the DataFrame and 'Class' column is missing for prediction\")\n",
    "else:\n",
    "    # Encode categorical variables\n",
    "    data = pd.get_dummies(data, columns=[categorical_column])\n",
    "\n",
    "# Handle missing values\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Drop unnecessary columns (e.g., 'Time')\n",
    "if 'Time' in data.columns:\n",
    "    data = data.drop(columns=['Time'])\n",
    "\n",
    "# Normalize numerical features\n",
    "if numerical_column in data.columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[[numerical_column]] = scaler.fit_transform(data[[numerical_column]])\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = shuffle(data)\n",
    "\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "782ekqkDsAZ8"
   },
   "source": [
    "Let's see how many genuine and limited fraudulent records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:58.944654Z",
     "start_time": "2024-08-06T16:07:58.939855Z"
    },
    "id": "owr545uamDd_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genuine records: 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the number of genuine and fraudulent records\n",
    "genuine_records = data[data['Class'] == 0]\n",
    "fraud_records = data[data['Class'] == 1]\n",
    "\n",
    "print(f\"Number of genuine records: {len(genuine_records)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p30Sel1l0zHe"
   },
   "source": [
    "### Task 2 - Data Preprocessing and Exploration\n",
    "\n",
    "*   Removing all the rows with `Nan` values\n",
    "*   Removing `Time` column\n",
    "*   Feature Scaling `Amount` column\n",
    "*   Split the data into features and labels\n",
    "*   Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyrwqwZWzclz"
   },
   "source": [
    "Removing the rows `Nan` values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:07:59.465306Z",
     "start_time": "2024-08-06T16:07:58.945641Z"
    },
    "id": "8oQ7BUhtzhJf"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['numerical_column'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [7], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Normalize numerical features\u001B[39;00m\n\u001B[1;32m      8\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler()\n\u001B[0;32m----> 9\u001B[0m data[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumerical_column\u001B[39m\u001B[38;5;124m'\u001B[39m]] \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumerical_column\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Shuffle the dataset\u001B[39;00m\n\u001B[1;32m     12\u001B[0m data \u001B[38;5;241m=\u001B[39m shuffle(data)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3810\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   3809\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 3810\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   3812\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6111\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[0;34m(self, key, axis_name)\u001B[0m\n\u001B[1;32m   6108\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6109\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 6111\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6113\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[1;32m   6114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[1;32m   6115\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6171\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[0;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[1;32m   6169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_interval_msg:\n\u001B[1;32m   6170\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 6171\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6173\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m   6174\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"None of [Index(['numerical_column'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Handling missing values\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Replace 'actual_categorical_column' with the correct column name from your dataset\n",
    "data = pd.get_dummies(data, columns=['correct_categorical_column'])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['numerical_column']] = scaler.fit_transform(data[['numerical_column']])\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = shuffle(data)\n",
    "\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3qtMCEcil5E"
   },
   "source": [
    "Removing Time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tBZhNbvinPu"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns (e.g., 'Time')\n",
    "if 'Time' in data.columns:\n",
    "    data = data.drop(columns=['Time'])\n",
    "    \n",
    "print(\"Time column removed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEChnc_GiAkd"
   },
   "source": [
    "Feature Scaling of Amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nykH6OHOiHZx"
   },
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['Amount']] = scaler.fit_transform(data[['Amount']])\n",
    "print(\"Amount column scaled!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvxgYLZJ0Fgg"
   },
   "source": [
    "Let's split the genuine and fraud records into separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KosQdd1yys4N"
   },
   "outputs": [],
   "source": [
    "# Split the data into genuine and fraud records\n",
    "genuine_records = data[data['Class'] == 0]\n",
    "fraud_records = data[data['Class'] == 1]\n",
    "\n",
    "print(f\"Number of genuine records: {len(genuine_records)}\")\n",
    "print(f\"Number of fraud records: {len(fraud_records)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AweLq-TNbyyy"
   },
   "source": [
    "Split the data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuNEggqWcY3S"
   },
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "\n",
    "print(\"Data split into features and labels!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uWA4S5HhY02"
   },
   "source": [
    "Data Exploration\n",
    "  - Apply PCA to reduce the dimensionality of features `X` into two dimensions\n",
    "  - Use a scatter plot to visualize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oc4JvsU9UaQH"
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce the dimensionality of features `X` into two dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2'])\n",
    "df_pca['Class'] = y.values\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='Class', s=10)\n",
    "plt.title('PCA of Genuine and Fraud Records')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkrSGlnZVmdc"
   },
   "source": [
    "Let's Use a scatter plot to visualize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAJ9F9LIVjs_"
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce the dimensionality of features `X` into two dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2'])\n",
    "df_pca['Class'] = y.values\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='Class', s=10)\n",
    "plt.title('PCA of Genuine and Fraud Records')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgFgpoOAS-8I"
   },
   "source": [
    "### Task 3 - Building the Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GS_SZ2gZKzd"
   },
   "source": [
    "Write a method to create the Generator model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56eOgnOdTA9n"
   },
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data, columns=['categorical_column'])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['numerical_column']] = scaler.fit_transform(data[['numerical_column']])\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = shuffle(data)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypwr3yYYaSq5"
   },
   "source": [
    "### Task 4 - Building the Discriminator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-78WbhfmaZ8t"
   },
   "source": [
    "Write a method to create the Discriminator model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfxnnV3uagl1"
   },
   "outputs": [],
   "source": [
    "# Ensure the data is in the right format and normalized for neural network training\n",
    "data['normalized_column'] = scaler.fit_transform(data[['column_to_normalize']])\n",
    "print(\"Data preprocessing for neural networks completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYIW2eB6bWg6"
   },
   "source": [
    "### Task 5 - Combine Generator and Discriminator models to Build The GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt2h8iDxzEp6"
   },
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "discriminator = create_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZCGFIKuzXxe"
   },
   "source": [
    "Let's create a method that generates synthetic data using the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPHMbppazcvb"
   },
   "outputs": [],
   "source": [
    "# Define the input dimension\n",
    "input_dim = 100  # Example value, adjust based on your dataset\n",
    "\n",
    "def create_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "discriminator = create_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXNZbInhmW5i"
   },
   "source": [
    "### Task 6 - Train and evaluate our GAN\n",
    "*    Defining some variables\n",
    "*    Creating our GAN\n",
    "*    Training the GAN\n",
    "*    Monitor the GAN performance using PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input dimension\n",
    "latent_dim = 100  # Example value, adjust based on your dataset\n",
    "\n",
    "def create_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(data.shape[1], activation='tanh'))  # Adjust the output layer based on your dataset\n",
    "    return model\n",
    "\n",
    "generator = create_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input dimension\n",
    "# input_dim = 100  # Example value, adjust based on your dataset\n",
    "\n",
    "\n",
    "# Create the discriminator\n",
    "discriminator = create_discriminator()\n",
    "\n",
    "# Create the generator\n",
    "generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "generator.summary()\n",
    "\n",
    "# Create the GAN\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "fake_data = generator(gan_input)\n",
    "gan_output = discriminator(fake_data)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'Class' column indicates fraud (1) and genuine (0) records\n",
    "data_fraud = data[data['Class'] == 1]\n",
    "\n",
    "# Assuming `data_fraud` is a DataFrame containing only fraud data\n",
    "# Drop the 'Class' column to get the features for fraud data\n",
    "real_fraud_data = data_fraud.drop(\"Class\", axis=1)\n",
    "\n",
    "# Freeze discriminator's weights\n",
    "discriminator.trainable = False\n",
    "\n",
    "# GAN input (noise) will be 100-dimensional vectors\n",
    "latent_dim = 100\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "fake_data = generator(gan_input)\n",
    "gan_output = discriminator(fake_data)\n",
    "\n",
    "# Create the GAN\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()\n",
    "\n",
    "def generate_synthetic_data(generator, num_samples):\n",
    "    # Generate noise\n",
    "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
    "\n",
    "    # Generate synthetic data\n",
    "    synthetic_data = generator.predict(noise)\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "def monitor_generator(generator):\n",
    "    # Initialize a PCA (Principal Component Analysis) object with 2 components\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # Drop the 'Class' column from the fraud dataset to get real data\n",
    "    real_fraud_data = data_fraud.drop(\"Class\", axis=1)\n",
    "\n",
    "    # Transform the real fraud data using PCA\n",
    "    transformed_data_real = pca.fit_transform(real_fraud_data.values)\n",
    "\n",
    "    # Create a DataFrame for the transformed real data and add a 'label' column with the value 'real'\n",
    "    df_real = pd.DataFrame(transformed_data_real)\n",
    "    df_real['label'] = \"real\"\n",
    "\n",
    "    # Generate synthetic fraud data using the provided generator and specify the number of samples (492 in this case)\n",
    "    synthetic_fraud_data = generate_synthetic_data(generator, 492)\n",
    "\n",
    "    # Transform the synthetic fraud data using PCA\n",
    "    transformed_data_fake = pca.fit_transform(synthetic_fraud_data)\n",
    "\n",
    "    # Create a DataFrame for the transformed fake data and add a 'label' column with the value 'fake'\n",
    "    df_fake = pd.DataFrame(transformed_data_fake)\n",
    "    df_fake['label'] = \"fake\"\n",
    "\n",
    "    # Concatenate the real and fake data DataFrames\n",
    "    df_combined = pd.concat([df_real, df_fake])\n",
    "\n",
    "    # Create a scatterplot to visualize the data points, using the first and second PCA components as x and y, respectively,\n",
    "    # and color points based on the 'label' column, with a size of 10\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=df_combined, x=0, y=1, hue='label', s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Grtte84z-NiK"
   },
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "def train_gan(epochs, batch_size, save_interval):\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        # Select a random half of real data\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size//2)\n",
    "        real_data = data[idx]\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = np.random.normal(0, 1, (batch_size//2, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size//2, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size//2, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_y = np.array([1] * batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "        # Save generated data at intervals\n",
    "        if epoch % save_interval == 0:\n",
    "            save_fake_data(epoch)\n",
    "            \n",
    "train_gan(epochs=10000, batch_size=64, save_interval=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCHApltUa3Yh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlGLX4Uk6l2C"
   },
   "source": [
    "### Task 7 - Generate synthetic data using the trained Generator\n",
    "\n",
    "*   Generate 1000 fradulent data points using the trained generator\n",
    "*   Compare the distribution of `real` and `synthetic` fradulent data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA1OkRU5UHOw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_gan(epochs, batch_size, save_interval):\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        # Select a random half of real data\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size//2)\n",
    "        real_data = data[idx]\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = np.random.normal(0, 1, (batch_size//2, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size//2, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size//2, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_y = np.array([1] * batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "        # Save generated data at intervals\n",
    "        if epoch % save_interval == 0:\n",
    "            save_fake_data(epoch)\n",
    "\n",
    "def save_fake_data(epoch):\n",
    "    noise = np.random.normal(0, 1, (100, latent_dim))\n",
    "    fake_data = generator.predict(noise)\n",
    "    # Save or plot the fake data\n",
    "\n",
    "train_gan(epochs=10000, batch_size=64, save_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "noise = np.random.normal(0, 1, (1000, latent_dim))\n",
    "synthetic_data = generator.predict(noise)\n",
    "\n",
    "# Add labels to synthetic data for comparison\n",
    "synthetic_data_df = pd.DataFrame(synthetic_data, columns=data.columns)\n",
    "synthetic_data_df['label'] = 1  # Label synthetic data as fraud\n",
    "\n",
    "# Combine real and synthetic data\n",
    "combined_df = pd.concat([data, synthetic_data_df])\n",
    "\n",
    "print(\"Synthetic data generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRp9QVL2VzrF"
   },
   "source": [
    "Checking the individual feature distribution of `synthetic` and `real` fraud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "VjR196XqKfVd",
    "outputId": "1414506b-7621-45c6-edbb-30f98af536f9"
   },
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    plt.figure()\n",
    "    fig = px.histogram(combined_df, color='label', x=col, barmode=\"overlay\", title=f'Feature {col}', width=640, height=500)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzZuOOGPHvZu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "noise = np.random.normal(0, 1, (1000, latent_dim))\n",
    "synthetic_data = generator.predict(noise)\n",
    "\n",
    "# Add labels to synthetic data for comparison\n",
    "synthetic_data_df = pd.DataFrame(synthetic_data, columns=data.columns)\n",
    "synthetic_data_df['label'] = 1  # Label synthetic data as fraud\n",
    "\n",
    "# Combine real and synthetic data\n",
    "combined_df = pd.concat([data, synthetic_data_df])\n",
    "\n",
    "print(\"Synthetic data generation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the combined dataset\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(combined_df.drop('label', axis=1))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_combined = pd.DataFrame(pca_result)\n",
    "df_combined['label'] = combined_df['label'].values\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_combined, x=0, y=1, hue='label', s=10)\n",
    "plt.title('PCA of Real and Synthetic Fraud Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    if col != 'label':\n",
    "        plt.figure()\n",
    "        fig = px.histogram(combined_df, color='label', x=col, barmode=\"overlay\", title=f'Feature {col}', width=640, height=500)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the generator\n",
    "\n",
    "monitor_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
